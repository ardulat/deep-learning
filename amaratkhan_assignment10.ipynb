{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "amaratkhan_assignment10.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "D2huqva-vllC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11da91e9-84ae-4150-ac5f-4032b2f371f4"
      },
      "cell_type": "code",
      "source": [
        "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n",
        "It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n",
        "(it's still underfitting at that point, though).\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LvwMJLVcvllK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1050Ez93vllO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de90b346-fb67-41d9-8266-23b75a9f9694"
      },
      "cell_type": "code",
      "source": [
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GSTHCszavllU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "# model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMQCQ-wpvllY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J99zu9Z3vllc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "46a11954-2522-4c1d-c71f-8a0c28f15767"
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test),\n",
        "          shuffle=True)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "50000/50000 [==============================] - 25s 507us/step - loss: 1.8669 - acc: 0.3284 - val_loss: 1.4338 - val_acc: 0.4854\n",
            "Epoch 2/10\n",
            "50000/50000 [==============================] - 23s 470us/step - loss: 1.4753 - acc: 0.4678 - val_loss: 1.3115 - val_acc: 0.5311\n",
            "Epoch 3/10\n",
            "50000/50000 [==============================] - 24s 474us/step - loss: 1.3260 - acc: 0.5267 - val_loss: 1.5099 - val_acc: 0.4835\n",
            "Epoch 4/10\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 1.2088 - acc: 0.5711 - val_loss: 1.0856 - val_acc: 0.6231\n",
            "Epoch 5/10\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 1.1179 - acc: 0.6045 - val_loss: 1.0171 - val_acc: 0.6416\n",
            "Epoch 6/10\n",
            "50000/50000 [==============================] - 24s 476us/step - loss: 1.0486 - acc: 0.6307 - val_loss: 1.0629 - val_acc: 0.6271\n",
            "Epoch 7/10\n",
            "50000/50000 [==============================] - 24s 476us/step - loss: 0.9849 - acc: 0.6551 - val_loss: 0.9547 - val_acc: 0.6611\n",
            "Epoch 8/10\n",
            "50000/50000 [==============================] - 24s 480us/step - loss: 0.9469 - acc: 0.6690 - val_loss: 0.8505 - val_acc: 0.7077\n",
            "Epoch 9/10\n",
            "50000/50000 [==============================] - 24s 475us/step - loss: 0.9050 - acc: 0.6843 - val_loss: 0.8251 - val_acc: 0.7151\n",
            "Epoch 10/10\n",
            "50000/50000 [==============================] - 24s 473us/step - loss: 0.8745 - acc: 0.6936 - val_loss: 0.7724 - val_acc: 0.7329\n",
            "10000/10000 [==============================] - 1s 137us/step\n",
            "Test loss: 0.7724247513771058\n",
            "Test accuracy: 0.7329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iRsnCk5ZesS0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Experiments:\n",
        "\n",
        "0. No BatchNormalization layer at all (by default). \n",
        "\n",
        "Hyperparameters: 10 epochs, optimizer - RMSProp, learning_rate=0.0001, loss - categorical_cross_entropy, batch_size=32, architecture - as it is now. Test set accuracy: **68%**\n",
        "\n",
        "1.   Add BatchNormalization layer everywhere before activation.\n",
        "\n",
        "Observation: causes test accuracy reduction because of overfitting. Test set accuracy: **11%**\n",
        "\n",
        "2.   Remove BatchNormalization from the last Dense layer, leave in convolutional part of the network. \n",
        "\n",
        "Observation: test set accuracy grows a lot in comparison to previous experiment. Test set accuracy: **71%**\n",
        "\n",
        "3. Add BatchNormalization layers only at the first block of convolution (before first dropout, 2 BatchNorms).\n",
        "\n",
        "Observation: the network behaves pretty much the same as in previous experiment. Test set accuracy: **71%**\n",
        "\n",
        "4. Add BatchNormalization layers only at the second block of convolution (2 BatchNorms).\n",
        "\n",
        "Observation: Behaves better than any previous network architectures. Test set accuracy: **73%**\n",
        "\n",
        "\n",
        "SOTA: **73%**\n"
      ]
    },
    {
      "metadata": {
        "id": "4VXRHKNovllf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}